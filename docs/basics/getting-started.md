# Quickstart 
<style>
  p {
    text-align: justify;
  }
</style>
This section provides basics steps to take before running PFD-kit
## Easy Install
PFD-kit can be built and installed form the source with `pip`:
```shell
pip install git+https://github.com/ruoyuwang1995nya/pfd-kit.git
```

## Job submission
PFD-kit comes with a simple CLI interface. For instance, a finetune workflow can be submitted using following command:
```shell
pfd submit finetune.json -t finetune
```
The `finetune.json` specifies imput parameters of the finetune task, whose details can be found in the `examples` directory. 
It should be noted that PFD-kit is built upon the [dflow](https://github.com/dptech-corp/dflow.git) package and utilized OPs from the [DPGEN2](https://github.com/deepmodeling/dpgen2.git) project, thus it is best to experience PFD-kit on the cloud-based [Bohrium](https://bohrium.dp.tech) platform, though local deployment is also possible.

## Tutorial: c-Si
This tutorial demmonstrates the general workflow of PFD-kit with a simple example of crystal Si in diamond structure. The relevant input files can be found at `/examples/silicon`. In this example, we need a efficient deep potential model of Si crytal with high accuracy for large scale atomic simulation, which can be easily generated from pretrained atomic through fine-tuning and distillation using PFD-kit. 


### Fine-tuning
In this example, we demonstrate the fine-tuning workflow with the latest DPA-3 pretrained model ([DPA-2.3.0-v3.0.0b4](https://www.aissquare.com/models/detail?pageType=models&name=DPA-2.3.0-v3.0.0b4&id=279)), a highly generalizable model with multiple predicting heads. One of the predicting heads named `Domains_SemiCond` is trained with DFT calculations generated by the `ABACUS` software and covers major types of semiconductor materials. Here, we may fine-tuning our VASP Si model from the `Domains_SemiCond` head. In fine-tuning step, you need to prepare the model file of DPA-2 pretrained model and its associated training script, which can be downloaded from [AIS square](https://www.aissquare.com/models/detail?pageType=models&name=DPA-2.3.0-v3.0.0b4&id=279). You also need to prepare the pseudopotential file for Si. The directory tree of fine-tune step is as follows:
```bash
examples/
├── c_Si/ 
│   ├── finetune 
│   |   ├── DPA2_medium_28_10M_rc0.pt 
│   │   ├── si_ft.json 
│   │   ├── input_torch_medium.json 
│   │   ├── INCAR.fp 
│   │   ├── POSCAR  
│   │   └── POTCAR
```
> Note: the DPA-2 pretrained model can be downloaded from AIS square


#### Preparing input script
Then we needs to prepare the input script file `si_ft.json`. The input script is a `JSON` file which contains multiple entries defining various aspects of the PFD workflow. To run this tutorial, we recommend to use the script which submit the VASP remote job through Slurm scheduler while doing everything else locally (You do need to install `deepmd-kit` locally). You can also run the workflow entirely on the local machine or run on the Bohrium cloud platform. You should input your log-in credentials submit the VASP jobs. 
```json
"step_configs": {
    "run_fp_config": {
    "template_config": {},
    "executor": {
        "type": "dispatcher",
        "host": "your host",
        "username": "your username",
        "password": "your password",
        "port": 22,
        "private_key_file": null,
        "remote_root": "/remote_root",
        "queue_name":"queue",    
        "machine_dict": {
                    "remote_profile": {
                        "timeout": 600
                        }},
        "resources_dict":{
            "source_list":[],
            "module_list": [],
            "custom_flags": []
                }},
        "template_slice_config": {
                "group_size": 1,
                "pool_size": 1
            }
        }
}
```


If you choose to run the PFD workflow on the `Bohrium` platform or custom Kubernetes services, the computing nodes need to be specified, which includes image name, machine types, etc. Images contain the software installation and neccesary dependencies, and a container would be initialized by the image upon execution. There is a default step setting in the `default_step_config`, which applies to all computation nodes if not otherwise specified. An example of running PFD workflow on `Bohrium` platform is provided. 

> **Note**: You need a valid Bohrium account to submit jobs to Bohrium platform.  

Then the parameters defining workflow tasks in the example `si_ft.json` file. Firstly, the task type (in this case "finetune") must be specified. Here we skip the initial data generation and training as the `Domains_SemiCond` branch already contains quite a lot information of crystal Si, and instead directly explore new Si configurations with the pretrained model. 
```json
"task":{
        "type":"finetune",
        "max_iter":5,
        "init_ft": false,
        "init_train": false
    }
```
The `inputs` section includes essential input parameters and model files.

```json
"inputs":{
    "base_model_path": "DPA2_medium_28_10M_beta4.pt",
    "init_confs":{
            "prefix": "./",
            "confs_paths": ["./pert_si32.extxyz"]
        },
    "init_fp_confs":{
            "prefix": "./",
            "confs_paths": []}
}
```

The `exploration` section defines the data exploration, it explores new Si configurations with molecular dynamics (MD) simulations using LAMMPS and add them into the training set for the next iteration. Here the exploration step only has one "stage", which generate new frame by running NPT simulation at 1000 K and at various perssure starting from one of the 5 perturbed Si structure. Each LAMMPS trajectory runs for 1000 steps, and a frame is extracted for subsequent labeling (i.e., DFT calculation). The exploration stage would run iteratively until convergence. The exploration converges when the root mean squre error (RMSE) of atomic force prediction falls below 0.06 eV/Angstrom.  

```json
"exploration": {
        "type": "ase",
        "config": {
            "calculator":"dp",
            "head":"Domains_SemiCond"
        },
        "stages": [
            [
                {
                "conf_idx": [0],
                "n_sample":1,
                "ens": "npt",
                "dt": 2,
                "nsteps": 2000,
                "temps": [1000],
                "press":[1,1000, 10000],
                "trj_freq": 10
                }
            ]
        ]
        
    },
```
> **Note**: when fine-tuning from a specific branch of deep potential model, `head` and `model_frozen_head` must be specified in the `exploration/config`.

The `select_confs` section specifies the max number of selected configuration for labeling. In this examples, the collected MD trajectories is first filtered by interatomic distance, and are then iteratevely added to the final dataset. 

```json
"select_confs":{
    "max_sel":60,
    "frame_filter": [
        {"type": "distance"}
    ],
    "h_filter": {
        "chunk_size":5,
        "_comment":"_entropy based filter"
        }
    },
```

The `fp` section defines DFT calculation settings. The path to the VASP input file as well as the pseudopotential file for each element are specified. The VASP command at `fp/run_config/command` should be configured according to the machine type for optimal performance.

```json
"fp": {
    "type": "vasp",
    "task_max": 50,
    "run_config": {
        "command": "source /opt/intel/oneapi/setvars.sh && mpirun -n 32 vasp_std"
        },
    "inputs_config": {
        "incar": "INCAR.fp",
        "pp_files": {
                "Si": "POTCAR"
            },
        "kspacing":0.2
        }
    },
```

The `train` section defines the type of pretrained model and the specific training configuration. To train a Deep Potential model, a seperate training script needs to be provided. 
```json
"train": {
    "type": "dp",
    "config": {
        "impl": "pytorch",
        "head":"Domains_SemiCond",
        },
    "template_script": "train.json",
    }
```
The final part is the `evaluate` section, where the model is tested against a test dataset randomly extracted from the last iteration.

```json
"evaluate": {
    "test_size": 0.3,
    "model":"dp",
    "head":"Domains_SemiCond",
    "_comment":"The percentage for test",
    "converge":{
        "type": "force_rmse",
        "RMSE": 0.06
        }
    },
```

#### Submit job
After modifying the input script, you can submit the fine-tune job using CLI:
```bash
pfd submit si_ft.json
```
With a successful submission, a workflow ID would be printed out to the console. You can check the workflow progression with the `status` command:
```bash
$ pfd status si_ft.json WORKFLOW_ID
+-------------+------------+------------+--------------+---------------+------------------+-------------+
|   iteration | type       |   criteria |   force_rmse |   energy_rmse |   selected_frame | converged   |
+=============+============+============+==============+===============+==================+=============+
|         000 | Force_RMSE |       0.06 |    0.0501339 |  0.00093      |               60 | True        |
+-------------+------------+------------+--------------+---------------+------------------+-------------+
```  

The final model can be downloaded to `~/si_ft_res/model/task.0000/model.ckpt.pt` using `download` command:
```bash
pfd download si_ft.json WORKFLOW_ID -p si_ft_res
```

The fine-tuned model exhibits much better accuracy on the test set, with a high energy prediction error of 0.002 eV/atom and a force prediction error of 0.056 eV/Angstrom, respectively. 

<div style="text-align: center;">
    <img src="../_static/ft_test.png" alt="Fig2" style="zoom: 100%;">
    <p style='font-size:1.0rem; text-align: center; font-weight:none'>Figure 2. Prediction error of the fine-tuned Si model.</p>
</div>

### Distillation
As mentioned, the network structure of the fine-tuned model is identical to that of the pre-trained model, hindering efficiency for large-scale atomic simulation. Fortunately, the complexity of the fine-tuned model is redundant given the chemical and configurational space associated with crystal Si. It is possible to transfer the knowledge of the fine-tuned to a lightweight model through a *"knowledge distillation"* process. The distillation workflow is essentially the same as fine-tuning, except that the DFT code is replaced by the fine-tuned model and the output model is trained from scratch.

In model distillation, the fine-tuned model is analogous to an AIMD engine generating training data (but much more efficient!). These data generated by the fine-tuned model would then be used to train the end model for large scale atomic simulation.

#### Preparing input script
The input script for model distillation is very similar to that of fine-tuning, except for a few differences.
```json
"task":{
    "type":"dist",
    "max_iter":5,
    },
"inputs":{
    "base_model_path":"model.ckp.pt"
    ...
    },
"train":{
    "type": "dp",
    ...
    "template_script": "dist_train.json"
},
"exploration":{
    ...
    }
"select_confs":{
        "max_sel":1500,
        "_comment":"you need much larger training data to train the model from scratch",
        "frame_filter": [
            {"type": "distance"}
        ],
        "h_filter": {
            "chunk_size":100,
            "_comment":"A larger chunk size is also desired"
        }
    },
```

#### Submit job
Submit job with the same `submit` command
```bash
pfd submit si_dist.json
```

The exploration process should converge after one iteration, where 1500 frames are generated and labeled by the fine-tuned model, among which 1350 frames are utilized as training set and the remaining frames as test set.
```bash
+-------------+------------+------------+--------------+---------------+---------+---------------------+------------------+-------------+
|   iteration | type       |   criteria |   force_rmse |   energy_rmse |   frame | unconverged_frame   |   selected_frame | converged   |
+=============+============+============+==============+===============+=========+=====================+==================+=============+
|         000 | Force_RMSE |       0.06 |    0.0462139 |   0.000844013 |       1 |                     |                1 | True        |
+-------------+------------+------------+--------------+---------------+---------+---------------------+------------------+-------------+
```

Upon successful completion, the end model can be generated and downloaded. The end model exhibits better accuracy than the pretrained model.
<div style="text-align: center;">
    <img src="../_static/dist_test.png" alt="Fig3" style="zoom: 100%;">
    <p style='font-size:1.0rem; text-align: center;font-weight:none'>Figure 3. Prediction error of the Si model generated through knowledge distillation.</p>
</div>

> **Note**: the whole process would take about 3 hours to complete (1.5 hour each for fine-tuning and distillation)

